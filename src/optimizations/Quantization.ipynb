{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPc6EpZlZLUu"
      },
      "outputs": [],
      "source": [
        "#!pip install google-cloud-secret-manager\n",
        "#!pip install -q wandb\n",
        "#!pip install transformers\n",
        "#!pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/cu111/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "#PANDAS\n",
        "import pandas as pd\n",
        "\n",
        "#DASK\n",
        "import dask.dataframe as dd\n",
        "\n",
        "#NUMPY\n",
        "import numpy as np\n",
        "\n",
        "#TORCH\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "#SKLEARN\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#TRANSFORMERS\n",
        "from transformers import BertTokenizer, DistilBertTokenizer, AutoTokenizer\n",
        "from transformers import BertForSequenceClassification, AutoModel, DistilBertForSequenceClassification, DebertaForSequenceClassification\n",
        "from transformers import AdamW\n",
        "\n",
        "#GOOGLE.CLOUD\n",
        "from google.cloud import secretmanager\n",
        "from google.cloud import storage\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available.\")\n",
        "else:\n",
        "    print(\"GPU is not available.\")"
      ],
      "metadata": {
        "id": "u9GpcCAJH5st",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbabc954-e3d1-4e19-f85f-aacdf3b7aeaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU is available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################\n",
        "######## CONSTANTS #########\n",
        "############################\n",
        "\n",
        "#name of the GCP project\n",
        "project_id = '222163854742'\n",
        "\n",
        "# key value of my secret -> this is the GOOGLE_APPLICATION_CREDENTIALS\n",
        "secrets = { \"project_secret\": \"harvardmlops_json\",\n",
        "            \"trainer_secret\": \"newtrainerjson\"}\n",
        "\n",
        "# DATA CONSTANTS\n",
        "#name of GCP DATA bucket\n",
        "bucket_name = \"harvardmlops\"\n",
        "\n",
        "#name of data bucket folder to read\n",
        "gold_folder = \"gold\"\n",
        "\n",
        "#location of GCP bucket/folder files\n",
        "files = f\"gs://{bucket_name}/{gold_folder}/*/*/*.gzip\"\n",
        "\n",
        "\n",
        "# MODEL CONSTANTS\n",
        "#name of GCP MODELS bucket\n",
        "models_bucket = \"dga-models\"\n",
        "\n",
        "#name of the best GENERAL model\n",
        "best_model = \"bert-base-uncased\"\n",
        "\n",
        "#name of best FINE-TUNED MODEL\n",
        "best_model_fine_tuned = \"bert_dga_classifier.pt\"\n",
        "\n",
        "# path to best fine tuned model\n",
        "best_fine_tuned_model_path = f\"gs://{models_bucket}/{best_model_fine_tuned}\"\n",
        "\n",
        "#name of model to be quantized\n",
        "best_model_quantized = \"quantized_bert_dga_classifier_0.pt\"\n",
        "\n",
        "#bucket to save models\n",
        "save_path = f\"gs://{models_bucket}\"\n",
        "\n",
        "#path where to find quantized model\n",
        "quantized_path = f\"gs://{models_bucket}/{best_model_quantized}\"\n",
        "# **** MODELS ****\n",
        "\n",
        "\n",
        "# LABELS\n",
        "#labels of the DGA actors\n",
        "actors = {'symmi':0, 'legit':1, 'ranbyus_v1':2, 'kraken_v1':3, 'not_dga':4, 'pushdo':5,\n",
        "          'ranbyus_v2':6, 'zeus-newgoz':7, 'locky':8, 'corebot':9, 'dyre':10, 'shiotob':11,\n",
        "          'proslikefan':12, 'nymaim':13, 'ramdo':14, 'necurs':15, 'tinba':16, 'vawtrak_v1':17,\n",
        "          'qadars':18, 'matsnu':19, 'fobber_v2':20, 'alureon':21, 'bedep':22, 'dircrypt':23,\n",
        "          'rovnix':24, 'sisron':25, 'cryptolocker':26, 'fobber_v1':27, 'chinad':28,\n",
        "          'padcrypt':29, 'simda':30}\n",
        "\n",
        "#the number of labels found in my classification problem\n",
        "num_labels = 31\n",
        "# *** LABELS ***\n",
        "\n",
        "#device needs to be cpu\n",
        "device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "WaA5cXYcgEk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################\n",
        "######### FUNCTIONS ########\n",
        "############################\n",
        "\n",
        "def get_token(type):\n",
        "\n",
        "    # Create a local secrets manager client:\n",
        "    client = secretmanager.SecretManagerServiceClient()\n",
        "\n",
        "    # get the secret name\n",
        "    secret_name = secrets.get(type)\n",
        "\n",
        "    # resource F string\n",
        "    resource_name = f\"projects/{project_id}/secrets/{secret_name}/versions/latest\"\n",
        "\n",
        "    # ask the client to get my secret\n",
        "    response = client.access_secret_version(request={\"name\": resource_name})\n",
        "\n",
        "    # decode the response\n",
        "    secret_string = response.payload.data.decode('UTF-8')\n",
        "\n",
        "    # token access\n",
        "    token = json.loads(secret_string)\n",
        "\n",
        "    return token\n",
        "\n",
        "\n",
        "def read(files:str, token):\n",
        "\n",
        "    #storage option paramenter\n",
        "    storage_options={'token': token}\n",
        "\n",
        "    #begin time\n",
        "    start = datetime.now()\n",
        "\n",
        "    #read files as parquest\n",
        "    df = dd.read_parquet(files, storage_options=storage_options)\n",
        "\n",
        "    #stop timing\n",
        "    end = datetime.now()\n",
        "\n",
        "    #give time result\n",
        "    print(f\"Read data from in GCP bucket in: {end-start}\")\n",
        "\n",
        "    #ensure domain is str\n",
        "    df['domains'] = df['domains'].astype(str)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_data(kind, frac=0.001, random_state=42):\n",
        "\n",
        "    #get trainer_secret token from secrets manager\n",
        "    token = get_token(kind)\n",
        "\n",
        "    #read files into a dask dataframe\n",
        "    df = read(files, token)\n",
        "\n",
        "    #convert to pandas DF for first transformation\n",
        "    pandas_df = df.compute()\n",
        "\n",
        "    #sample dataframe\n",
        "    sampled_df = pandas_df.sample(frac=frac, random_state=random_state)\n",
        "\n",
        "    return sampled_df\n",
        "\n",
        "\n",
        "def tokenize_data(model_name, data, predict=False):\n",
        "\n",
        "    # init tokenizer\n",
        "    if model_name == 'bert-base-uncased':\n",
        "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    elif model_name == 'distilbert-base-uncased':\n",
        "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    elif model_name == 'microsoft/deberta-base':\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"Model not found\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # tokenize the data/domains\n",
        "    tokenized_text = [tokenizer.encode(domain, truncation=True, add_special_tokens=True, max_length=20, pad_to_max_length=True) for domain in data['domains']]\n",
        "\n",
        "    if predict:\n",
        "        return tokenized_text\n",
        "\n",
        "    else:\n",
        "        # map str labels to int\n",
        "        labels = data['actor'].map(actors)\n",
        "\n",
        "        return  tokenized_text, labels\n",
        "\n",
        "\n",
        "def split_data(tokenized_text, labels, batch_size, random_state=42):\n",
        "\n",
        "    # Split the data to get test data - 20 percent\n",
        "    X_train, X_test, y_train, y_test = train_test_split(tokenized_text, labels, test_size=0.2, random_state=random_state)\n",
        "\n",
        "    # Split again to get train and validation data\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=random_state)\n",
        "\n",
        "    #X_train and y_train LongTensor\n",
        "    X_train, y_train = torch.LongTensor(X_train), torch.LongTensor(np.array(y_train))\n",
        "\n",
        "    #X_val and y_val LongTensor\n",
        "    X_val, y_val = torch.LongTensor(X_val),torch.LongTensor(np.array(y_val))\n",
        "\n",
        "    #X_test and y_test LongTensor\n",
        "    X_test, y_test = torch.LongTensor(X_test), torch.LongTensor(np.array(y_test))\n",
        "\n",
        "    #training dataloader\n",
        "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    #validation dataloader\n",
        "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    #test dataloader\n",
        "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "\n",
        "def get_model(model_name):\n",
        "\n",
        "    #MODEL 1\n",
        "    if model_name == 'bert-base-uncased':\n",
        "        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "    #MODEL 2\n",
        "    elif model_name == 'distilbert-base-uncased':\n",
        "        model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "    #MODEL 3\n",
        "    elif model_name == 'microsoft/deberta-base':\n",
        "        model = DebertaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"Model Not Found\")\n",
        "        sys.exit(0)\n",
        "\n",
        "    #send to cpu\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize a GCS client\n",
        "    client = storage.Client()\n",
        "\n",
        "    #local_path\n",
        "    local_model_path = f\"{best_model_fine_tuned}\"\n",
        "\n",
        "    # Download the model from GCS\n",
        "    bucket_name, blob_name = best_fine_tuned_model_path.replace('gs://', '').split('/', 1)\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.download_to_filename(local_model_path)\n",
        "\n",
        "    #load the state dictionary of the fine-tune tuned model found in path\n",
        "    model.load_state_dict(torch.load(local_model_path, map_location=device))\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "mrmir_2gbapo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################\n",
        "######### PROCESS ##########\n",
        "############################\n",
        "\n",
        "# RETRIEVE DATA - sample 1 percent of data\n",
        "data = get_data(\"trainer_secret\", frac=0.01)\n",
        "\n",
        "# GET FINE-TUNED MODEL AND OPTIMIZER\n",
        "#model, optimizer = get_model(args.model_name, data, args.lr)\n",
        "model = get_model(best_model)\n",
        "\n",
        "# TOKENIZE DATA AND RETRIEVE LABELS\n",
        "#domains, labels = tokenize_data(args.model_name, data)\n",
        "domains, labels = tokenize_data(best_model, data)\n",
        "\n",
        "# SPLIT DATA\n",
        "#train_loader, val_loader, test_loader = split_data(domains, labels, args.batch_size)\n",
        "train_loader, val_loader, test_loader = split_data(domains, labels, 64)"
      ],
      "metadata": {
        "id": "XwCjYxPRcVIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the FINE-TUNED model on test data\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, labels = [data.to(device) for data in batch]\n",
        "        output = model(input_ids)\n",
        "        predictions = torch.argmax(output.logits, dim=1)\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1mNC52vcblO",
        "outputId": "7d33e5f6-b586-4a15-dbd3-02622dbebc24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 90.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "print(quantized_model)"
      ],
      "metadata": {
        "id": "-dgNVO3XcflQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')\n",
        "\n",
        "print_size_of_model(model)\n",
        "print_size_of_model(quantized_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xcO5YMjcm93",
        "outputId": "64677c4e-f1fe-4373-f3a1-e3f22518e9ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size (MB): 438.08923\n",
            "Size (MB): 181.501674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantized Evaluation\n",
        "quantized_model.to(torch.device('cpu'))\n",
        "quantized_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids, labels = [data.to(torch.device('cpu')) for data in batch]\n",
        "        output = quantized_model(input_ids)\n",
        "        predictions = torch.argmax(output.logits, dim=1)\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)"
      ],
      "metadata": {
        "id": "CvlAYQNOc0iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model.named_modules"
      ],
      "metadata": {
        "id": "fuOeTCeNwsza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAFjB7HOkuIh",
        "outputId": "7550cf6f-3687-4460-8171-d82d6d733b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 87.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SAVE THE MODEL\n",
        "torch.save({\n",
        "    'model_state_dict': quantized_model.state_dict(),\n",
        "    'quantization_config': quantized_model.config,\n",
        "}, best_model_quantized)"
      ],
      "metadata": {
        "id": "EcxNWW4UrLFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Load the saved model state_dict and quantization configuration\n",
        "checkpoint = torch.load(path)\n",
        "loaded_quantized_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "loaded_quantized_model.qconfig = checkpoint['quantization_config']\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "loaded_quantized_model.eval()\n",
        "\n",
        "# Move the loaded quantized model to the desired device\n",
        "device = torch.device(\"cpu\")  # Change to your desired device\n",
        "loaded_quantized_model = loaded_quantized_model.to(device)"
      ],
      "metadata": {
        "id": "iSpK8gEG0BYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantized Evaluation\n",
        "loaded_quantized_model.to(torch.device('cpu'))\n",
        "loaded_quantized_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids, labels = [data.to(torch.device('cpu')) for data in batch]\n",
        "        output = loaded_quantized_model(input_ids)\n",
        "        predictions = torch.argmax(output.logits, dim=1)\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)"
      ],
      "metadata": {
        "id": "szNsKYOwwihW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx5Sq1Yq5uvA",
        "outputId": "0e3b8667-dd93-478a-e6ba-d1f51a673106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 87.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXAMPLE OF HOW TO ENCODE AND RUN FOR A LIST OF DOMAINS\n",
        "\n",
        "\n",
        "# EXAMPLE DOMAINS\n",
        "domains = [\"cnn.com\", \"espn.com\", \"dfjdfjdf.ru\", \"doraku.com\", \"kidfhkhjgareuspde.it\"]\n",
        "domains =pd.DataFrame(domains, columns=['domains'])\n",
        "\n",
        "#Tokenize the DOMAINS\n",
        "tokenized_text = tokenize_data('bert-base-uncased', domains, predict=True)\n",
        "#[tokenizer.encode(domain, truncation=True, add_special_tokens=True, max_length=20, pad_to_max_length=True) for domain in domains]\n",
        "\n",
        "# Setup the Inputs\n",
        "input = torch.LongTensor(tokenized_text)\n",
        "\n",
        "# Eval the model\n",
        "loaded_quantized_model.eval()\n",
        "\n",
        "# Retreive the outputs\n",
        "output = loaded_quantized_model(input)\n",
        "\n",
        "# Get the predictions\n",
        "predictions = torch.argmax(output.logits, dim=1)\n",
        "\n",
        "# Print the predictions\n",
        "predictions.tolist()"
      ],
      "metadata": {
        "id": "qernwSo_SCbS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
